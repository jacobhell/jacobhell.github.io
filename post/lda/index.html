<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>Implementing LDA in Python</title><link rel=stylesheet href=/css/style.css></head><body><header>================<br>== <a href>Jacob Hell</a> ==<br>================<div style=float:right></div><br><p><nav><a href=/><b>Start</b></a>.
<a href=/posts/><b>Posts</b></a>.
<a href=/runs/><b>Runs</b></a>.
<a href=/links/><b>Links</b></a>.
<a href=/index.xml><b>Feed</b></a>.</nav></p></header><main><article><h1>Implementing LDA in Python</h1><b><time>30.01.2021</time></b>
<a href=/tags/python>python</a>
<a href=/tags/nlp>nlp</a>
<a href=/tags/ml>ml</a><div><h2 id=definition>Definition</h2><p>Latent Dirichlet Allocation (LDA) is a modelling technique used for NLP.</p><p>I&rsquo;m following along on this <a href=https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial>kaggle notebook</a> to learn and explain these two techniques.</p><h2 id=code>Code</h2><p>I have uploaded the jupyter file <a href=https://github.com/jacobhell/learning-ml/blob/main/nlp-starter/main.ipynb>here</a>.</p><p>And that&rsquo;s how you do it. I&rsquo;m not entirely sure how it works yet either, but I am learning!</p></div></article></main><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=/post/fascination-indie-web/>My Recent Fascination with the IndieWeb</a></li><li><a href=/post/5-19-21/>5-19-2021</a></li><li><a href=/post/5-18-21/>5-18-2021</a></li><li><a href=/post/5-17-21/>5-17-2021</a></li><li><a href=/post/5-16-21/>5-16-2021</a></li></ul></div></div></aside><footer><p>&copy; 2021 <a href><b>Jacob Hell</b></a>.
<a href=https://github.com/jacobhell><b>Github</b></a>.
<a href=https://linkedin.com/in/jacobhell><b>LinkedIn</b></a>.</p></footer></body></html>