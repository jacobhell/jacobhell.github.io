<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spark on Jacob Hell's Blog</title><link>https://jacobhell.github.io/tags/spark/</link><description>Recent content in spark on Jacob Hell's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 18 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://jacobhell.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Running Spark Applications on Dockerized Spark</title><link>https://jacobhell.github.io/post/running-spark-applications-docker/</link><pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate><guid>https://jacobhell.github.io/post/running-spark-applications-docker/</guid><description>&lt;p>Sparking joy in Docker.&lt;/p>
&lt;p>I&amp;rsquo;m getting into data engineering stuff. The biggest thing in data engineering right now is Spark. Spark lets you perform distributed processes. I consider it to be the Hadoop successor, since it&amp;rsquo;s so much faster.&lt;/p>
&lt;p>Requirements:&lt;/p>
&lt;ol>
&lt;li>Docker&lt;/li>
&lt;li>Scala version 2.12.12 (Spark doesn&amp;rsquo;t work with 2.13.* at the time of this writing)&lt;/li>
&lt;/ol>
&lt;h2 id="getting-dockerized-spark-running">Getting Dockerized Spark Running&lt;/h2>
&lt;p>First, pull the docker image &lt;code>bitnami/spark&lt;/code>, using this command:&lt;/p>
&lt;pre>&lt;code>docker pull bitnami/spark
&lt;/code>&lt;/pre>&lt;p>It&amp;rsquo;s going to take awhile to download, so I suggest pulling up some Rick and Morty. Two episodes should do the trick.&lt;/p>
&lt;p>Then run it, using this command:&lt;/p>
&lt;pre>&lt;code>docker run -d bitnami/spark
&lt;/code>&lt;/pre>&lt;p>&lt;code>-d&lt;/code> runs in detached mode, so you retain access to your terminal emulator. Docker prints out the hash, keep this handy.&lt;/p>
&lt;h2 id="packaging-a-jar-using-sbt">Packaging a jar using sbt&lt;/h2>
&lt;p>Go to a Scala program that you want to run in Spark. If you are in need of one, use this:&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-scala" data-lang="scala">&lt;span style="color:#75715e">/* SimpleApp.scala */&lt;/span>
&lt;span style="color:#66d9ef">import&lt;/span> org.apache.spark.sql.SparkSession
&lt;span style="color:#66d9ef">object&lt;/span> &lt;span style="color:#a6e22e">SimpleApp&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;span style="color:#66d9ef">def&lt;/span> main&lt;span style="color:#f92672">(&lt;/span>args&lt;span style="color:#66d9ef">:&lt;/span> &lt;span style="color:#66d9ef">Array&lt;/span>&lt;span style="color:#f92672">[&lt;/span>&lt;span style="color:#66d9ef">String&lt;/span>&lt;span style="color:#f92672">])&lt;/span> &lt;span style="color:#f92672">{&lt;/span>
&lt;span style="color:#66d9ef">val&lt;/span> logFile &lt;span style="color:#66d9ef">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;YOUR_SPARK_HOME/README.md&amp;#34;&lt;/span> &lt;span style="color:#75715e">// Should be some file on your system
&lt;/span>&lt;span style="color:#75715e">&lt;/span> &lt;span style="color:#66d9ef">val&lt;/span> spark &lt;span style="color:#66d9ef">=&lt;/span> &lt;span style="color:#a6e22e">SparkSession&lt;/span>&lt;span style="color:#f92672">.&lt;/span>builder&lt;span style="color:#f92672">.&lt;/span>appName&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Simple Application&amp;#34;&lt;/span>&lt;span style="color:#f92672">).&lt;/span>getOrCreate&lt;span style="color:#f92672">()&lt;/span>
&lt;span style="color:#66d9ef">val&lt;/span> logData &lt;span style="color:#66d9ef">=&lt;/span> spark&lt;span style="color:#f92672">.&lt;/span>read&lt;span style="color:#f92672">.&lt;/span>textFile&lt;span style="color:#f92672">(&lt;/span>logFile&lt;span style="color:#f92672">).&lt;/span>cache&lt;span style="color:#f92672">()&lt;/span>
&lt;span style="color:#66d9ef">val&lt;/span> numAs &lt;span style="color:#66d9ef">=&lt;/span> logData&lt;span style="color:#f92672">.&lt;/span>filter&lt;span style="color:#f92672">(&lt;/span>line &lt;span style="color:#66d9ef">=&amp;gt;&lt;/span> line&lt;span style="color:#f92672">.&lt;/span>contains&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;a&amp;#34;&lt;/span>&lt;span style="color:#f92672">)).&lt;/span>count&lt;span style="color:#f92672">()&lt;/span>
&lt;span style="color:#66d9ef">val&lt;/span> numBs &lt;span style="color:#66d9ef">=&lt;/span> logData&lt;span style="color:#f92672">.&lt;/span>filter&lt;span style="color:#f92672">(&lt;/span>line &lt;span style="color:#66d9ef">=&amp;gt;&lt;/span> line&lt;span style="color:#f92672">.&lt;/span>contains&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">&amp;#34;b&amp;#34;&lt;/span>&lt;span style="color:#f92672">)).&lt;/span>count&lt;span style="color:#f92672">()&lt;/span>
println&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#e6db74">s&amp;#34;Lines with a: &lt;/span>&lt;span style="color:#e6db74">$numAs&lt;/span>&lt;span style="color:#e6db74">, Lines with b: &lt;/span>&lt;span style="color:#e6db74">$numBs&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#f92672">)&lt;/span>
spark&lt;span style="color:#f92672">.&lt;/span>stop&lt;span style="color:#f92672">()&lt;/span>
&lt;span style="color:#f92672">}&lt;/span>
&lt;span style="color:#f92672">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>I took this snippet from &lt;a href="https://spark.apache.org/docs/latest/quick-start.html">here&lt;/a>.&lt;/p>
&lt;p>Package the program using &lt;code>sbt&lt;/code>:&lt;/p>
&lt;pre>&lt;code>sbt package
&lt;/code>&lt;/pre>&lt;h2 id="uploading-the-jar-and-running-the-spark-application">Uploading the jar and Running the Spark Application&lt;/h2>
&lt;p>To upload the jar to the docker container, we are going to use the &lt;code>docker cp&lt;/code> command. This is where you need the hash.&lt;/p>
&lt;p>Run this command:&lt;/p>
&lt;pre>&lt;code>docker cp &amp;lt;jar_file_on_your_machine&amp;gt;.jar &amp;lt;hash&amp;gt;:/opt/bitnami/spark/app.jar
&lt;/code>&lt;/pre>&lt;p>Then, shell into your docker container using the command:&lt;/p>
&lt;pre>&lt;code>docker exec -it &amp;lt;hash&amp;gt; bash
&lt;/code>&lt;/pre>&lt;p>Lastly, in your spark docker container, run this command:&lt;/p>
&lt;pre>&lt;code>bin/spark-submit --class &amp;quot;SimpleApp&amp;quot; --master local[4] app.jar
&lt;/code>&lt;/pre>&lt;p>If you see something similar to &lt;strong>Lines with a: 46, Lines with b: 23&lt;/strong>, then good job, it works! You are ready for more Spark adventures.&lt;/p></description></item></channel></rss>