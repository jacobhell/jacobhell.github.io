<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Jacob Hell</title><link>/tags/ml/</link><description>Recent content in ml on Jacob Hell</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 31 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Implementing Pipeline from the SciKit Learn Module</title><link>/post/pipeline-nlp/</link><pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate><guid>/post/pipeline-nlp/</guid><description>I&amp;rsquo;m continuing learning NLP. In this notebook, I give an example of the Pipeline class for preprocessing and model fitting.
I&amp;rsquo;m using the fetch_20newsgroups dataset from sklearn.datasets.
You can find the code here.</description></item><item><title>Implementing LDA in Python</title><link>/post/lda/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>/post/lda/</guid><description>Definition Latent Dirichlet Allocation (LDA) is a modelling technique used for NLP.
I&amp;rsquo;m following along on this kaggle notebook to learn and explain these two techniques.
Code I have uploaded the jupyter file here.
And that&amp;rsquo;s how you do it. I&amp;rsquo;m not entirely sure how it works yet either, but I am learning!</description></item></channel></rss>