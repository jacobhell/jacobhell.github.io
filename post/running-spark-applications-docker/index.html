<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><style type=text/css>body{font-family:monospace}</style><title>Running Spark Applications on Dockerized Spark</title><link rel=stylesheet href=/css/style.css></head><body><header>=======================<br>== <a href=https://jacobhell.github.io/>Jacob Hell's Blog</a> ==<br>=======================<div style=float:right></div><br><p><nav><a href=/><b>Start</b></a>.
<a href=/posts/><b>Posts</b></a>.
<a href=/categories/><b>Categories</b></a>.
<a href=/tags/><b>Tags</b></a>.
<a href=/index.xml><b>Feed</b></a>.</nav></p></header><main><article><h1>Running Spark Applications on Dockerized Spark</h1><b><time>18.01.2021 00:00</time></b>
<a href=/tags/spark>spark</a>
<a href=/tags/docker>docker</a>
<a href=/tags/scala>scala</a><div><p>Sparking joy in Docker.</p><p>I&rsquo;m getting into data engineering stuff. The biggest thing in data engineering right now is Spark. Spark lets you perform distributed processes. I consider it to be the Hadoop successor, since it&rsquo;s so much faster.</p><p>Requirements:</p><ol><li>Docker</li><li>Scala version 2.12.12 (Spark doesn&rsquo;t work with 2.13.* at the time of this writing)</li></ol><h2 id=getting-dockerized-spark-running>Getting Dockerized Spark Running</h2><p>First, pull the docker image <code>bitnami/spark</code>, using this command:</p><pre><code>docker pull bitnami/spark
</code></pre><p>It&rsquo;s going to take awhile to download, so I suggest pulling up some Rick and Morty. Two episodes should do the trick.</p><p>Then run it, using this command:</p><pre><code>docker run -d bitnami/spark
</code></pre><p><code>-d</code> runs in detached mode, so you retain access to your terminal emulator. Docker prints out the hash, keep this handy.</p><h2 id=packaging-a-jar-using-sbt>Packaging a jar using sbt</h2><p>Go to a Scala program that you want to run in Spark. If you are in need of one, use this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#75715e>/* SimpleApp.scala */</span>
<span style=color:#66d9ef>import</span> org.apache.spark.sql.SparkSession

<span style=color:#66d9ef>object</span> <span style=color:#a6e22e>SimpleApp</span> <span style=color:#f92672>{</span>
  <span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span> <span style=color:#f92672>{</span>
    <span style=color:#66d9ef>val</span> logFile <span style=color:#66d9ef>=</span> <span style=color:#e6db74>&#34;YOUR_SPARK_HOME/README.md&#34;</span> <span style=color:#75715e>// Should be some file on your system
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>val</span> spark <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>appName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Simple Application&#34;</span><span style=color:#f92672>).</span>getOrCreate<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> logData <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(</span>logFile<span style=color:#f92672>).</span>cache<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> numAs <span style=color:#66d9ef>=</span> logData<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>line <span style=color:#66d9ef>=&gt;</span> line<span style=color:#f92672>.</span>contains<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;a&#34;</span><span style=color:#f92672>)).</span>count<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> numBs <span style=color:#66d9ef>=</span> logData<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>line <span style=color:#66d9ef>=&gt;</span> line<span style=color:#f92672>.</span>contains<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;b&#34;</span><span style=color:#f92672>)).</span>count<span style=color:#f92672>()</span>
    println<span style=color:#f92672>(</span><span style=color:#e6db74>s&#34;Lines with a: </span><span style=color:#e6db74>$numAs</span><span style=color:#e6db74>, Lines with b: </span><span style=color:#e6db74>$numBs</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>)</span>
    spark<span style=color:#f92672>.</span>stop<span style=color:#f92672>()</span>
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><p>I took this snippet from <a href=https://spark.apache.org/docs/latest/quick-start.html>here</a>.</p><p>Package the program using <code>sbt</code>:</p><pre><code>sbt package
</code></pre><h2 id=uploading-the-jar-and-running-the-spark-application>Uploading the jar and Running the Spark Application</h2><p>To upload the jar to the docker container, we are going to use the <code>docker cp</code> command. This is where you need the hash.</p><p>Run this command:</p><pre><code>docker cp &lt;jar_file_on_your_machine&gt;.jar &lt;hash&gt;:/opt/bitnami/spark/app.jar
</code></pre><p>Then, shell into your docker container using the command:</p><pre><code>docker exec -it &lt;hash&gt; bash
</code></pre><p>Lastly, in your spark docker container, run this command:</p><pre><code>bin/spark-submit --class &quot;SimpleApp&quot; --master local[4] app.jar
</code></pre><p>If you see something similar to <strong>Lines with a: 46, Lines with b: 23</strong>, then good job, it works! You are ready for more Spark adventures.</p></div></article></main><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=/post/java-16/>What's New in Java 16</a></li><li><a href=/post/gaming-pc-under-250/>How to Build a Gaming PC Under $250</a></li><li><a href=/post/reliability-scalability-maintainability/>What Are Reliability, Scalability, and Maintainability in Distributed Systems?</a></li><li><a href=/post/pipeline-nlp/>Implementing Pipeline from the SciKit Learn Module</a></li><li><a href=/post/setting-up-pihole/>Setting Up my PiHole, a Short Story</a></li></ul></div></div></aside><footer><p>&copy; 2021 <a href=https://jacobhell.github.io/><b>Jacob Hell's Blog</b></a>.
<a href=https://github.com/jacobhell><b>Github</b></a>.
<a href=https://linkedin.com/in/jacobhell><b>LinkedIn</b></a>.</p></footer></body></html>