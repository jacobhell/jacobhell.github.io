<!doctype html><html lang=en><head><meta charset=utf-8><title>Running Spark Applications on Dockerized Spark</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-161403382-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-161403382-1')</script><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><link rel=alternate type=application/rss+xml href=https://jacobhell.github.io/index.xml title="Jacob Hell's Blog"><link id=dark-mode-theme rel=stylesheet href=https://jacobhell.github.io/css/dark.css><link rel=stylesheet href=https://jacobhell.github.io/fontawesome/css/all.min.css><script src=https://jacobhell.github.io/js/bundle.js></script><script src=https://jacobhell.github.io/js/instantpage.js type=module defer></script><meta name=generator content="Hugo 0.81.0"></head><body><header><nav class=navbar><div class=nav><ul class=nav-links><li><a href=/about/ name=About><i class="fas fa-user fa-lg"></i></a></li><li><a href=/tags name=Tags><i class="fas fa-tag fa-lg"></i></a></li><li><a href=/search name=Search><i class="fas fa-search fa-lg"></i></a></li><li><a href=/index.xml name=RSS><i class="fas fa-rss"></i></a></li></ul></div></nav><div class=intro-header><div class=container><div class=post-heading><h1>Running Spark Applications on Dockerized Spark</h1><span class=meta-post><i class="fa fa-calendar-alt"></i>&nbsp;Jan 18, 2021</span></div></div></div></header><div class=container role=main><article class=article class=blog-post><p>Sparking joy in Docker.</p><p>I&rsquo;m getting into data engineering stuff. The biggest thing in data engineering right now is Spark. Spark lets you perform distributed processes. I consider it to be the Hadoop successor, since it&rsquo;s so much faster.</p><p>Requirements:</p><ol><li>Docker</li><li>Scala version 2.12.12 (Spark doesn&rsquo;t work with 2.13.* at the time of this writing)</li></ol><h2 id=getting-dockerized-spark-running>Getting Dockerized Spark Running</h2><p>First, pull the docker image <code>bitnami/spark</code>, using this command:</p><pre><code>docker pull bitnami/spark
</code></pre><p>It&rsquo;s going to take awhile to download, so I suggest pulling up some Rick and Morty. Two episodes should do the trick.</p><p>Then run it, using this command:</p><pre><code>docker run -d bitnami/spark
</code></pre><p><code>-d</code> runs in detached mode, so you retain access to your terminal emulator. Docker prints out the hash, keep this handy.</p><h2 id=packaging-a-jar-using-sbt>Packaging a jar using sbt</h2><p>Go to a Scala program that you want to run in Spark. If you are in need of one, use this:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#75715e>/* SimpleApp.scala */</span>
<span style=color:#66d9ef>import</span> org.apache.spark.sql.SparkSession

<span style=color:#66d9ef>object</span> <span style=color:#a6e22e>SimpleApp</span> <span style=color:#f92672>{</span>
  <span style=color:#66d9ef>def</span> main<span style=color:#f92672>(</span>args<span style=color:#66d9ef>:</span> <span style=color:#66d9ef>Array</span><span style=color:#f92672>[</span><span style=color:#66d9ef>String</span><span style=color:#f92672>])</span> <span style=color:#f92672>{</span>
    <span style=color:#66d9ef>val</span> logFile <span style=color:#66d9ef>=</span> <span style=color:#e6db74>&#34;YOUR_SPARK_HOME/README.md&#34;</span> <span style=color:#75715e>// Should be some file on your system
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>val</span> spark <span style=color:#66d9ef>=</span> <span style=color:#a6e22e>SparkSession</span><span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>appName<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;Simple Application&#34;</span><span style=color:#f92672>).</span>getOrCreate<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> logData <span style=color:#66d9ef>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>textFile<span style=color:#f92672>(</span>logFile<span style=color:#f92672>).</span>cache<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> numAs <span style=color:#66d9ef>=</span> logData<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>line <span style=color:#66d9ef>=&gt;</span> line<span style=color:#f92672>.</span>contains<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;a&#34;</span><span style=color:#f92672>)).</span>count<span style=color:#f92672>()</span>
    <span style=color:#66d9ef>val</span> numBs <span style=color:#66d9ef>=</span> logData<span style=color:#f92672>.</span>filter<span style=color:#f92672>(</span>line <span style=color:#66d9ef>=&gt;</span> line<span style=color:#f92672>.</span>contains<span style=color:#f92672>(</span><span style=color:#e6db74>&#34;b&#34;</span><span style=color:#f92672>)).</span>count<span style=color:#f92672>()</span>
    println<span style=color:#f92672>(</span><span style=color:#e6db74>s&#34;Lines with a: </span><span style=color:#e6db74>$numAs</span><span style=color:#e6db74>, Lines with b: </span><span style=color:#e6db74>$numBs</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>)</span>
    spark<span style=color:#f92672>.</span>stop<span style=color:#f92672>()</span>
  <span style=color:#f92672>}</span>
<span style=color:#f92672>}</span>
</code></pre></div><p>I took this snippet from <a href=https://spark.apache.org/docs/latest/quick-start.html>here</a>.</p><p>Package the program using <code>sbt</code>:</p><pre><code>sbt package
</code></pre><h2 id=uploading-the-jar-and-running-the-spark-application>Uploading the jar and Running the Spark Application</h2><p>To upload the jar to the docker container, we are going to use the <code>docker cp</code> command. This is where you need the hash.</p><p>Run this command:</p><pre><code>docker cp &lt;jar_file_on_your_machine&gt;.jar &lt;hash&gt;:/opt/bitnami/spark/app.jar
</code></pre><p>Then, shell into your docker container using the command:</p><pre><code>docker exec -it &lt;hash&gt; bash
</code></pre><p>Lastly, in your spark docker container, run this command:</p><pre><code>bin/spark-submit --class &quot;SimpleApp&quot; --master local[4] app.jar
</code></pre><p>If you see something similar to <strong>Lines with a: 46, Lines with b: 23</strong>, then good job, it works! You are ready for more Spark adventures.</p><div class=blog-tags><a href=https://jacobhell.github.io//tags/spark/>spark</a>&nbsp;
<a href=https://jacobhell.github.io//tags/docker/>docker</a>&nbsp;
<a href=https://jacobhell.github.io//tags/scala/>scala</a>&nbsp;</div></article></div><footer><div class=container><p class="credits copyright"><a href=https://jacobhell.github.io/about>Jacob Hell</a>
&nbsp;&copy;
2021
&nbsp;/&nbsp;
<a href=https://jacobhell.github.io/>Jacob Hell's Blog</a>
&nbsp;&ndash;&nbsp;
<i class="fas fa-moon" id=dark-mode-toggle></i><p class="credits theme-by">Powered By <a href=https://gohugo.io>Hugo</a>&nbsp;Theme <a href=https://github.com/matsuyoshi30/harbor>Harbor</a></p></p></div></footer></body></html>